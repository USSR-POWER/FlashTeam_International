{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2c0b24-37fc-49af-8c7b-c7b6ca7cd198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./environments/hack/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./environments/hack/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: sentence_transformers in ./environments/hack/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: python-docx in ./environments/hack/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: bitsandbytes in ./environments/hack/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in ./environments/hack/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: transformers in ./environments/hack/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: protobuf in ./environments/hack/lib/python3.10/site-packages (5.28.3)\n",
      "Requirement already satisfied: sentencepiece in ./environments/hack/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: openpyxl in ./environments/hack/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./environments/hack/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./environments/hack/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./environments/hack/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./environments/hack/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./environments/hack/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./environments/hack/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./environments/hack/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: Pillow in ./environments/hack/lib/python3.10/site-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: tqdm in ./environments/hack/lib/python3.10/site-packages (from sentence_transformers) (4.67.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./environments/hack/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./environments/hack/lib/python3.10/site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in ./environments/hack/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in ./environments/hack/lib/python3.10/site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: pyyaml in ./environments/hack/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./environments/hack/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in ./environments/hack/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./environments/hack/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./environments/hack/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./environments/hack/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./environments/hack/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./environments/hack/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: et-xmlfile in ./environments/hack/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./environments/hack/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in ./environments/hack/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.1.0 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: networkx in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./environments/hack/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./environments/hack/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./environments/hack/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./environments/hack/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./environments/hack/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./environments/hack/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./environments/hack/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn sentence_transformers python-docx bitsandbytes accelerate transformers protobuf sentencepiece openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f329151-0c55-4e4a-ad0b-cee1e5395af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели MPNet для эмбеддингов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка LLM модели...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee7b08a87614bdd8c6acf6a44adbe1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели загружены.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from transformers import AutoTokenizer, MPNetModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Загрузка моделей для работы с эмбеддингами\n",
    "print(\"Загрузка модели MPNet для эмбеддингов...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mpnet-base\")\n",
    "model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\")\n",
    "sts_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Настройка для LLM модели, которая квантована в 4-бит для скорости\n",
    "print(\"Загрузка LLM модели...\")\n",
    "\n",
    "hf_token = \"<ВАШ ТОКЕН HUGGING FACE>\" #Нужно для загрузки Mistral. Перед созданием токена нужно получить доступ к модели тут https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "if llm_tokenizer.pad_token_id is None:\n",
    "    llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": \"cuda:0\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Модели загружены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b18aa9c-8830-4bbb-9f09-9ff1ec1d4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-28561 Уровень: LC\n",
      "Время обработки для пары UC-28561.docx и SSTS-28561.docx: 12.90 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-31523 Уровень: LC\n",
      "Время обработки для пары UC-31523.docx и SSTS-31523.docx: 9.60 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-25957 Уровень: PC\n",
      "Время обработки для пары UC-25957.docx и SSTS-25957.docx: 13.84 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-8604 Уровень: PC\n",
      "Время обработки для пары UC-8604.docx и SSTS-8604.docx: 10.09 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-8800 Уровень: LC\n",
      "Время обработки для пары UC-8800.docx и SSTS-8800.docx: 16.67 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-26771 Уровень: LC\n",
      "Время обработки для пары UC-26771.docx и SSTS-26771.docx: 12.45 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-26161 Уровень: LC\n",
      "Время обработки для пары UC-26161.docx и SSTS-26161.docx: 10.89 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-11467 Уровень: FC\n",
      "Время обработки для пары UC-11467.docx и SSTS-11467.docx: 8.49 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-8692 Уровень: LC\n",
      "Время обработки для пары UC-8692.docx и SSTS-8692.docx: 17.38 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-30371 Уровень: LC\n",
      "Время обработки для пары UC-30371.docx и SSTS-30371.docx: 20.64 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка для uc-6583 Уровень: LC\n",
      "Время обработки для пары UC-6583.docx и SSTS-6583.docx: 15.12 секунд\n",
      "Нет соответствующего ssts файла\n",
      "Проверка для uc-26160 Уровень: LC\n",
      "Время обработки для пары UC-26160.docx и SSTS-26160.docx: 0.00 секунд\n",
      "Среднее время обработки одного файла: 12.34 секунд\n",
      "CSV-файл 'submission_format.csv' создан.\n",
      "Excel-файл 'submission_format.xlsx' создан.\n"
     ]
    }
   ],
   "source": [
    "# Функция для извлечения текста из docx файла\n",
    "def extract_text_from_docx(filepath):\n",
    "    \"\"\"\n",
    "    Извлекает полный текст из .docx файла и объединяет его в одну строку.\n",
    "    Возвращает весь текст документа в виде строки, где каждый параграф отделен переносом строки.\n",
    "    \"\"\"\n",
    "    doc = Document(filepath)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Функция для генерации эмбеддинга текста с использованием модели MPNet\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"\n",
    "    Генерирует эмбеддинг для заданного текста, используя модель MPNet.\n",
    "    Текст сначала токенизируется и обрабатывается моделью, затем возвращается среднее значение эмбеддингов по всем токенам.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "\n",
    "def get_sts_embedding(text):\n",
    "    \"\"\"\n",
    "    Генерирует эмбеддинг для текста с использованием модели SentenceTransformer.\n",
    "    Возвращает тензор, представляющий векторное представление текста.\n",
    "    \"\"\"\n",
    "    return torch.tensor(sts_model.encode(text, convert_to_tensor=False))\n",
    "\n",
    "def extract_text_structured(file_path):\n",
    "    \"\"\"\n",
    "    Извлекает и структурирует текст из .docx файла. \n",
    "    Разделяет текст на секции, основываясь на форматировании документа (напр., нумерации и заголовках).\n",
    "    Возвращает словарь, где ключи — это имена секций, а значения — их содержимое.\n",
    "    \"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    section_positions = [(match.start(), match.group().strip()) for match in re.finditer(r\"(.*?[:])|([0-9]+\\.)\", text)]\n",
    "    structured_text = {}\n",
    "    for idx, (start_pos, section_name) in enumerate(section_positions):\n",
    "        end_pos = section_positions[idx + 1][0] if idx + 1 < len(section_positions) else len(text)\n",
    "        section_content = text[start_pos:end_pos].strip()\n",
    "        structured_text[section_name.lower().strip(\":\")] = section_content\n",
    "    return structured_text\n",
    "\n",
    "def match_sections_with_threshold(text_uc, text_ssts, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Сопоставляет секции UC и SSTS документов, используя эмбеддинги и заданный порог схожести.\n",
    "    Возвращает словарь с сопоставленными секциями и уровнем схожести для каждой из них.\n",
    "    \"\"\"\n",
    "    matched_sections = {}\n",
    "    uc_content_embeddings = {section: get_text_embedding(content) for section, content in text_uc.items()}\n",
    "    uc_content_sts_embeddings = {section: get_sts_embedding(content) for section, content in text_uc.items()}\n",
    "    \n",
    "    for uc_section, uc_embedding in uc_content_embeddings.items():\n",
    "        similarities = []\n",
    "        \n",
    "        for ssts_section, ssts_content in text_ssts.items():\n",
    "            ssts_embedding = get_text_embedding(ssts_content)\n",
    "            ssts_sts_embedding = get_sts_embedding(ssts_content)\n",
    "            \n",
    "            # Проверяем размерность эмбеддингов, если совпадает — считаем схожесть\n",
    "            if uc_embedding.dim() == ssts_embedding.dim():\n",
    "                cosine_sim = cosine_similarity(\n",
    "                    uc_embedding.unsqueeze(0).numpy(),\n",
    "                    ssts_embedding.unsqueeze(0).numpy()\n",
    "                ).item()\n",
    "            else:\n",
    "                cosine_sim = 0\n",
    "            \n",
    "            # Считаем косинусное сходство с помощью SentenceTransformer\n",
    "            sts_sim = torch.nn.functional.cosine_similarity(\n",
    "                uc_content_sts_embeddings[uc_section].unsqueeze(0),\n",
    "                ssts_sts_embedding.unsqueeze(0),\n",
    "                dim=1\n",
    "            ).item()\n",
    "            \n",
    "            combined_sim = (cosine_sim + sts_sim) / 2\n",
    "            similarities.append((ssts_section, combined_sim))\n",
    "        \n",
    "        # Находим лучшее совпадение с наибольшей схожестью\n",
    "        best_match = max(similarities, key=lambda x: x[1], default=(None, 0))\n",
    "        \n",
    "        if best_match[1] > threshold:\n",
    "            matched_sections[uc_section] = {\n",
    "                'matched_section': best_match[0],\n",
    "                'content_uc': text_uc[uc_section],\n",
    "                'content_ssts': text_ssts.get(best_match[0], \"No content\"),\n",
    "                'similarity': best_match[1]\n",
    "            }\n",
    "        else:\n",
    "            matched_sections[uc_section] = {\n",
    "                'matched_section': None,\n",
    "                'content_uc': text_uc[uc_section],\n",
    "                'content_ssts': None,\n",
    "                'similarity': best_match[1]\n",
    "            }\n",
    "\n",
    "    return matched_sections\n",
    "\n",
    "def compute_similarity_metrics(matched_sections):\n",
    "    \"\"\"\n",
    "    Вычисляет метрики сходства, такие как среднее, медианное и стандартное отклонение.\n",
    "    Возвращает словарь с рассчитанными статистическими метриками для переданных значений сходства.\n",
    "    \"\"\"\n",
    "    similarities = [\n",
    "        info['similarity'] for info in matched_sections.values()\n",
    "        if isinstance(info, dict) and 'similarity' in info\n",
    "    ]\n",
    "    return {\n",
    "        \"mean_similarity\": np.mean(similarities),\n",
    "        \"median_similarity\": np.median(similarities),\n",
    "        \"std_deviation\": np.std(similarities),\n",
    "        \"min_similarity\": np.min(similarities),\n",
    "        \"max_similarity\": np.max(similarities)\n",
    "    }\n",
    "\n",
    "def evaluate_compliance(metrics):\n",
    "    \"\"\"\n",
    "    Оценивает соответствие на основе метрик сходства.\n",
    "    Возвращает уровень соответствия (например, 'FC' для полного соответствия или 'NC' для отсутствия соответствия) \n",
    "    на основе заданных порогов для метрик.\n",
    "    \"\"\"\n",
    "    mean_similarity = metrics[\"mean_similarity\"]\n",
    "    median_similarity = metrics[\"median_similarity\"]\n",
    "    min_similarity = metrics[\"min_similarity\"]\n",
    "    std_deviation = metrics[\"std_deviation\"]\n",
    "    \n",
    "    if mean_similarity >= 0.6 and median_similarity >= 0.6 and min_similarity >= 0.6 and std_deviation <= 0.1:\n",
    "        return \"FC\"\n",
    "    elif mean_similarity >= 0.6 and median_similarity >= 0.6 and min_similarity >= 0.35 and std_deviation <= 0.2:\n",
    "        return \"LC\"\n",
    "    elif mean_similarity >= 0.55 and median_similarity >= 0.55 and min_similarity >= 0.3 and std_deviation <= 0.25:\n",
    "        return \"PC\"\n",
    "    else:\n",
    "        return \"NC\"\n",
    "\n",
    "def generate_difference_text(uc_text, ssts_text):\n",
    "    prompt = f\"\"\"You are a comparison model designed to analyze technical specifications from UC and SSTS documents. Your task is to identify only the most important differences in the functional details described in the SSTS Specification compared to the UC Specification.\n",
    "    UC Specification:\n",
    "    [{uc_text}]\n",
    "\n",
    "    SSTS Specification:\n",
    "    [{ssts_text}]\n",
    "\n",
    "    Provide a very short list of the most important differences where SSTS misses certain details or differs in functionality from UC.\n",
    "    If there are no functional differences, answer \"-\"\n",
    "    Your answer never exceeds 5 sentences.\n",
    "    List the differences found:\n",
    "\"\"\"\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs = {key: value.to(\"cuda:0\") for key, value in inputs.items()}\n",
    "    generated_ids = llm_model.generate(**inputs, max_new_tokens=450, do_sample=True)\n",
    "    decoded = llm_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"List the differences found:\" in decoded:\n",
    "        return decoded.split(\"List the differences found:\")[-1].strip()\n",
    "    return \"-\"\n",
    "\n",
    "# Доки вместе с файлом\n",
    "directory = 'train_data'\n",
    "results = []\n",
    "times = []  # Для хранения времени обработки каждого файла\n",
    "\n",
    "# Измененный код для извлечения `name`\n",
    "for uc_filename in os.listdir(directory):\n",
    "    if uc_filename.startswith(\"UC-\") and uc_filename.endswith(\".docx\"):\n",
    "        ssts_filename = uc_filename.replace(\"UC-\", \"SSTS-\")\n",
    "        ssts_filepath = os.path.join(directory, ssts_filename)\n",
    "        uc_filepath = os.path.join(directory, uc_filename)\n",
    "        \n",
    "        start_time = time.time()  # Начало замера времени для файла\n",
    "\n",
    "        # Проверка на наличие файла SSTS\n",
    "        if os.path.exists(ssts_filepath):\n",
    "            # Извлечение и обработка текста\n",
    "            text_uc = extract_text_structured(uc_filepath)\n",
    "            text_ssts = extract_text_structured(ssts_filepath)\n",
    "            matched_sections = match_sections_with_threshold(text_uc, text_ssts)\n",
    "            metrics = compute_similarity_metrics(matched_sections)\n",
    "            compliance = evaluate_compliance(metrics)\n",
    "            \n",
    "            # Генерация описания различий\n",
    "            uc_text = extract_text_from_docx(uc_filepath)\n",
    "            ssts_text = extract_text_from_docx(ssts_filepath)\n",
    "            differences = generate_difference_text(uc_text, ssts_text)\n",
    "\n",
    "            # Извлечение `name` из содержимого UC-файла с проверкой формата\n",
    "            name_line = uc_text.splitlines()[0]  # Извлечение первой строки\n",
    "            if re.match(r'^\\[.*?\\]', name_line):  # Проверка, есть ли идентификатор в квадратных скобках\n",
    "                name = re.sub(r'^\\[.*?\\]\\s*', '', name_line).strip()  # Удаление идентификатора\n",
    "            else:\n",
    "                name = number  # Если идентификатора нет, используем `number`\n",
    "\n",
    "            # Добавление в результаты\n",
    "            number = uc_filename.split(\"-\")[1].split(\".\")[0]\n",
    "            results.append({\n",
    "                \"Number\": number,\n",
    "                \"Name\": name,\n",
    "                \"Differences\": differences,\n",
    "                \"Description\": \"\",  \n",
    "                \"Complience Level\": compliance\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # Если пара SSTS не найдена\n",
    "            number = uc_filename.split(\"-\")[1].split(\".\")[0]\n",
    "            results.append({\n",
    "                \"Number\": number,\n",
    "                \"Name\": number,  # Используем `number` в качестве `name`\n",
    "                \"Differences\": \"\",\n",
    "                \"Description\": \"\",\n",
    "                \"Complience Level\": \"NA\"\n",
    "            })\n",
    "            print('Нет соответствующего ssts файла')\n",
    "\n",
    "\n",
    "\n",
    "        # Конец замера времени для файла\n",
    "        end_time = time.time()\n",
    "        file_time = end_time - start_time\n",
    "        times.append(file_time)  # Сохранение времени обработки\n",
    "        print(f'Проверка для uc-{number} Уровень: {compliance}')\n",
    "        print(f\"Время обработки для пары {uc_filename} и {ssts_filename}: {file_time:.2f} секунд\")\n",
    "\n",
    "\n",
    "# Подсчет и вывод среднего времени обработки\n",
    "average_time_per_file = np.mean(times)\n",
    "print(f\"Среднее время обработки одного файла: {average_time_per_file:.2f} секунд\")\n",
    "\n",
    "# Создание CSV-файла\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('submission_format.csv', index=False)\n",
    "print(\"CSV-файл 'submission_format.csv' создан.\")\n",
    "\n",
    "# Создание Excel-файла\n",
    "results_df.to_excel('submission_format.xlsx', index=False)\n",
    "print(\"Excel-файл 'submission_format.xlsx' создан.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7f868-8a52-4e07-af49-0489b59b72be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5b27-bf1c-4912-94d0-e79bf3d41f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
